{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T11:47:43.704637Z",
     "start_time": "2025-03-24T11:47:41.858122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from stuff import Dataset\n",
    "\n",
    "a = pd.read_csv('data/AppGallery.csv')\n",
    "p = pd.read_csv('data/Purchasing.csv')"
   ],
   "id": "bee8fd24b2728727",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T11:47:45.421497Z",
     "start_time": "2025-03-24T11:47:45.376020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a_pre: pd.DataFrame = a[[\"Interaction content\", \"Type 2\", \"Type 3\", \"Type 4\"]].copy()\n",
    "\n",
    "a_pre.rename({\n",
    "    \"Interaction content\": \"content\",\n",
    "    \"Type 2\": \"type2\",\n",
    "    \"Type 3\": \"type3\",\n",
    "    \"Type 4\": \"type4\"\n",
    "}, axis=1, inplace=True)\n",
    "\n",
    "a_pre = a_pre[a_pre['content'].notna()]\n",
    "a_pre.sort_values(by=[\"type2\", \"type3\", \"type4\"], inplace=True)\n",
    "\n",
    "a_pre"
   ],
   "id": "a934eefb123ce95f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                              content       type2  \\\n",
       "0   Mxxxxx@xxxx.com Отправлено с моего телефона Hu...      Others   \n",
       "12  Huawi health On Mon, 5 Sept *****(PHONE), 1:40...      Others   \n",
       "15  Description In the area where I am (*****(LOC)...      Others   \n",
       "16  Description What's up Contact info *****(PHONE...      Others   \n",
       "17  Описание What's up Информация за контакт *****...      Others   \n",
       "..                                                ...         ...   \n",
       "8   Sure, the country of my app gallery and my cou...  Suggestion   \n",
       "86  Estimado Hugo Brito, Recomendamos que nos cont...  Suggestion   \n",
       "87  Bom dia, Não tenho computador Obter Outlook pa...  Suggestion   \n",
       "88  Estimado Hugo Brito, Obrigado pelo seu regress...  Suggestion   \n",
       "89  Onde vejo a identificação Huawei. Hugo Brito *...  Suggestion   \n",
       "\n",
       "                        type3                           type4  \n",
       "0                         NaN                             NaN  \n",
       "12                        NaN                             NaN  \n",
       "15                        NaN                             NaN  \n",
       "16                        NaN                             NaN  \n",
       "17                        NaN                             NaN  \n",
       "..                        ...                             ...  \n",
       "8   VIP / Offers / Promotions  Offers / Vouchers / Promotions  \n",
       "86  VIP / Offers / Promotions  Offers / Vouchers / Promotions  \n",
       "87  VIP / Offers / Promotions  Offers / Vouchers / Promotions  \n",
       "88  VIP / Offers / Promotions  Offers / Vouchers / Promotions  \n",
       "89  VIP / Offers / Promotions  Offers / Vouchers / Promotions  \n",
       "\n",
       "[120 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>type2</th>\n",
       "      <th>type3</th>\n",
       "      <th>type4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mxxxxx@xxxx.com Отправлено с моего телефона Hu...</td>\n",
       "      <td>Others</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Huawi health On Mon, 5 Sept *****(PHONE), 1:40...</td>\n",
       "      <td>Others</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Description In the area where I am (*****(LOC)...</td>\n",
       "      <td>Others</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Description What's up Contact info *****(PHONE...</td>\n",
       "      <td>Others</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Описание What's up Информация за контакт *****...</td>\n",
       "      <td>Others</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sure, the country of my app gallery and my cou...</td>\n",
       "      <td>Suggestion</td>\n",
       "      <td>VIP / Offers / Promotions</td>\n",
       "      <td>Offers / Vouchers / Promotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Estimado Hugo Brito, Recomendamos que nos cont...</td>\n",
       "      <td>Suggestion</td>\n",
       "      <td>VIP / Offers / Promotions</td>\n",
       "      <td>Offers / Vouchers / Promotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Bom dia, Não tenho computador Obter Outlook pa...</td>\n",
       "      <td>Suggestion</td>\n",
       "      <td>VIP / Offers / Promotions</td>\n",
       "      <td>Offers / Vouchers / Promotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Estimado Hugo Brito, Obrigado pelo seu regress...</td>\n",
       "      <td>Suggestion</td>\n",
       "      <td>VIP / Offers / Promotions</td>\n",
       "      <td>Offers / Vouchers / Promotions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Onde vejo a identificação Huawei. Hugo Brito *...</td>\n",
       "      <td>Suggestion</td>\n",
       "      <td>VIP / Offers / Promotions</td>\n",
       "      <td>Offers / Vouchers / Promotions</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T11:47:54.410977Z",
     "start_time": "2025-03-24T11:47:54.315994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = Dataset(a_pre, 'content')\n",
    "data.tuplefy(('type2', 'type3', 'type4'))\n",
    "data.vectorise('content', 10000)\n",
    "data.split()"
   ],
   "id": "eea45e016f3b9fe0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T11:48:02.801266Z",
     "start_time": "2025-03-24T11:48:02.792045Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "59a7dbcc6dce8939",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T11:12:11.416221Z",
     "start_time": "2025-03-24T11:12:11.270258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data.vectorise('content')\n",
    "data.df.head()"
   ],
   "id": "e1ff91853808a1b5",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvectorise\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcontent\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m data\u001B[38;5;241m.\u001B[39mdf\u001B[38;5;241m.\u001B[39mhead()\n",
      "File \u001B[0;32m~/PycharmProjects/CA_multi-label_email_classification/stuff/data.py:48\u001B[0m, in \u001B[0;36mDataset.vectorise\u001B[0;34m(self, col, max_features)\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;124;03mvectorises text data\u001B[39;00m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;124;03m:param col: str column in the df to vectorise\u001B[39;00m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;124;03m:param max_features: int max features\u001B[39;00m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;124;03m:return: None\u001B[39;00m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvectorisers[col] \u001B[38;5;241m=\u001B[39m TfidfVectorizer(max_features\u001B[38;5;241m=\u001B[39mmax_features)\n\u001B[0;32m---> 48\u001B[0m tmp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvectorisers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcol\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcol\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m tfidf_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(tmp\u001B[38;5;241m.\u001B[39mtoarray(), columns\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvectorisers[col]\u001B[38;5;241m.\u001B[39mget_feature_names_out())\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf\u001B[38;5;241m.\u001B[39mdrop(col, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m), tfidf_df], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/CA_multi-label_email_classification/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:2104\u001B[0m, in \u001B[0;36mTfidfVectorizer.fit_transform\u001B[0;34m(self, raw_documents, y)\u001B[0m\n\u001B[1;32m   2097\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_params()\n\u001B[1;32m   2098\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfidf \u001B[38;5;241m=\u001B[39m TfidfTransformer(\n\u001B[1;32m   2099\u001B[0m     norm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm,\n\u001B[1;32m   2100\u001B[0m     use_idf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_idf,\n\u001B[1;32m   2101\u001B[0m     smooth_idf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msmooth_idf,\n\u001B[1;32m   2102\u001B[0m     sublinear_tf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msublinear_tf,\n\u001B[1;32m   2103\u001B[0m )\n\u001B[0;32m-> 2104\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2105\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfidf\u001B[38;5;241m.\u001B[39mfit(X)\n\u001B[1;32m   2106\u001B[0m \u001B[38;5;66;03m# X is already a transformed view of raw_documents so\u001B[39;00m\n\u001B[1;32m   2107\u001B[0m \u001B[38;5;66;03m# we set copy to False\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/CA_multi-label_email_classification/.venv/lib/python3.9/site-packages/sklearn/base.py:1389\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1382\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1384\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1385\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1386\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1387\u001B[0m     )\n\u001B[1;32m   1388\u001B[0m ):\n\u001B[0;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/CA_multi-label_email_classification/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1376\u001B[0m, in \u001B[0;36mCountVectorizer.fit_transform\u001B[0;34m(self, raw_documents, y)\u001B[0m\n\u001B[1;32m   1368\u001B[0m             warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m   1369\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUpper case characters found in\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1370\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m vocabulary while \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlowercase\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1371\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is True. These entries will not\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1372\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m be matched with any documents\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1373\u001B[0m             )\n\u001B[1;32m   1374\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m-> 1376\u001B[0m vocabulary, X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_count_vocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfixed_vocabulary_\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1378\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbinary:\n\u001B[1;32m   1379\u001B[0m     X\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mfill(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/CA_multi-label_email_classification/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1263\u001B[0m, in \u001B[0;36mCountVectorizer._count_vocab\u001B[0;34m(self, raw_documents, fixed_vocab)\u001B[0m\n\u001B[1;32m   1261\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m raw_documents:\n\u001B[1;32m   1262\u001B[0m     feature_counter \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m-> 1263\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m feature \u001B[38;5;129;01min\u001B[39;00m \u001B[43manalyze\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m   1264\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1265\u001B[0m             feature_idx \u001B[38;5;241m=\u001B[39m vocabulary[feature]\n",
      "File \u001B[0;32m~/PycharmProjects/CA_multi-label_email_classification/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:99\u001B[0m, in \u001B[0;36m_analyze\u001B[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001B[0m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001B[39;00m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;124;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001B[39;00m\n\u001B[1;32m     79\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;124;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001B[39;00m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m decoder \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 99\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m analyzer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    101\u001B[0m     doc \u001B[38;5;241m=\u001B[39m analyzer(doc)\n",
      "File \u001B[0;32m~/PycharmProjects/CA_multi-label_email_classification/.venv/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:232\u001B[0m, in \u001B[0;36m_VectorizerMixin.decode\u001B[0;34m(self, doc)\u001B[0m\n\u001B[1;32m    229\u001B[0m     doc \u001B[38;5;241m=\u001B[39m doc\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoding, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecode_error)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m doc \u001B[38;5;129;01mis\u001B[39;00m np\u001B[38;5;241m.\u001B[39mnan:\n\u001B[0;32m--> 232\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    233\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    234\u001B[0m     )\n\u001B[1;32m    236\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m doc\n",
      "\u001B[0;31mValueError\u001B[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import numpy as np\n",
   "id": "5f8822eeb90ec1b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import numpy as np\n",
   "id": "651a013b17bfc2e4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
